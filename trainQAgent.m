% **************************************************************
% * Hochschule Wismar
% * Research Group Computational Engineering & Automation (CEA)
% * T. Pawletta, J. Bartelt
% * 
% * This example script shows how the Q-Agent from the 
% * Reinforcement Learning Toolbox can be trained on a SimEvents
% * model. The model 'Prodline' contains a server that can 
% * produce two types of parts. Each type of part is fed into 
% * a separate queue after completion. The goal of the training
% * is for the agent to learn a strategy that leads to the most
% * balanced queue contents possible.
% **************************************************************

rng(1);

MaxEpisodes = 2e4;

%% learning parameter
alpha = 0.1;
gamma = 0.9;
epsilon0 = 1;
epsilonMin = 0.01;
epsilonDecayEpisodes = 1e4;
lambda = log(epsilon0/epsilonMin) / epsilonDecayEpisodes;

%% simulation options / constants over all episodes
tFinal = 8*60;

SimOptions.tMax = tFinal; % minutes
SimOptions.ProcessingTime_Job1 = 1;
SimOptions.ProcessingTime_Job2 = 3;
SimOptions.RetoolingTime = 3;
SimOptions.SaleTime_Job1 = 10;
SimOptions.SaleTime_Job2 = 5;
SimOptions.MaxQueueLength = 30; % limits the decoded state
SimOptions.MaxDeltaQueue = 10; % may be used in the acceptor

%% create Agent
obsInfo = rlFiniteSetSpec(1:(2*31^2)); % limited observation set
actInfo = rlFiniteSetSpec(1:2); % available server settings -> job type 1/2

table = rlTable(obsInfo, actInfo);
critic = rlQValueFunction(table, obsInfo, actInfo);

agentOpts = rlQAgentOptions;
agentOpts.CriticOptimizerOptions.LearnRate = alpha;
agentOpts.DiscountFactor = gamma;
agentOpts.EpsilonGreedyExploration.Epsilon = epsilon0;
agentOpts.EpsilonGreedyExploration.EpsilonMin = epsilonMin;
agentOpts.EpsilonGreedyExploration.EpsilonDecay = lambda;
agentOpts.SampleTime = -1;

Agent = rlQAgent(critic, agentOpts);

%% create environment
model = 'Prodline_EF_RL';
load_system(model);
mws = get_param(model, "ModelWorkspace");
mws.assignin('SimOptions', SimOptions);

env = rlSimulinkEnv(model, ...
    'Prodline_EF_RL/TriggeredSubsystem/RLAgent', 'UseFastRestart','on');

%% training
% note on the workaround used:
%   A initial message is required to start the 'Prodline' and generate the
%   first 'part'. The message is generated by an Entity Generator.
%   When using 'FastRestart ', a bug causes the messages generated at time t=0 
%   to be counted in the generator, which leads to an error message after 5000 episodes.
%   To avoid this, 'FastRestart' is switched on and off periodically.

trainOpts = rlTrainingOptions;
trainOpts.MaxEpisodes = 10; % workaround 
trainOpts.MaxStepsPerEpisode = 1e4;
trainOpts.ScoreAveragingWindowLength = 30;
trainOpts.StopTrainingCriteria = 'AverageReward';
trainOpts.StopTrainingValue = 18e4;

% train first 10 episodes to get trainingStats
trainingStats = train(Agent,env,trainOpts);

while trainingStats.EpisodeIndex(end) < MaxEpisodes
    trainingStats.TrainingOptions.MaxEpisodes = min(MaxEpisodes, trainingStats.EpisodeIndex(end)+5000);
    
    % train at most 5000 episodes to avoid "entities at time 0" bug
    trainingStats = train(Agent,env,trainingStats);

    % detect stop button in RL Episode Manager
    if trainingStats.EpisodeIndex(end) < trainingStats.TrainingOptions.MaxEpisodes
       break;
    end
    
    % reset entity counter by turning off and on FastRestart
    set_param(model, "FastRestart", 'off');
    set_param(model, "FastRestart", 'on');
end
